{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Segmentation_PyTorch_Unet++_ResNet34).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzOHPX0-aTdf"
      },
      "source": [
        "import os, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as album"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikwqKXiJabFz"
      },
      "source": [
        "!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn74YgrJa0yF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqelejyUa37O"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Machine_Learning/Sand') \n",
        "!pwd\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_c9vChPav3w"
      },
      "source": [
        "#Defining train / val / test directories\n",
        "DATA_DIR = 'datasets/'\n",
        "\n",
        "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n",
        "\n",
        "list = os.listdir(y_train_dir) # dir is your directory path\n",
        "number_files = len(list)\n",
        "print('len_train: ',number_files)\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'validation')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'validation_labels')\n",
        "\n",
        "list = os.listdir(y_valid_dir) # dir is your directory path\n",
        "number_files = len(list)\n",
        "print('len_valid: ',number_files)\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "y_test_dir = os.path.join(DATA_DIR, 'test_labels')\n",
        "\n",
        "list = os.listdir(y_test_dir) # dir is your directory path\n",
        "number_files = len(list)\n",
        "print('len: ',number_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8oECAlZa5SQ"
      },
      "source": [
        "#All dataset classes and their corresponding RGB values in labels\n",
        "class_dict = pd.read_csv(\"label_class_dict.csv\")\n",
        "# Get class names\n",
        "class_names = class_dict['name'].tolist()\n",
        "# Get class RGB values\n",
        "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
        "\n",
        "print('All dataset classes and their corresponding RGB values in labels:')\n",
        "print('Class Names: ', class_names)\n",
        "print('Class RGB values: ', class_rgb_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtwIRhBGa-6F"
      },
      "source": [
        "#Shortlist specific classes to segment\n",
        "# Useful to shortlist specific classes in datasets with large number of classes\n",
        "select_classes = ['pore', 'grain', 'water', 'air-grain', 'air-water', 'grain-water']\n",
        "\n",
        "# Get RGB values of required classes\n",
        "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
        "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
        "\n",
        "print('Selected classes and their corresponding RGB values in labels:')\n",
        "print('Selected Names: ', select_classes)\n",
        "print('Selected Class RGB values: ', select_class_rgb_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKjSrfScbEKD"
      },
      "source": [
        "#Helper functions for viz. & one-hot encoding/decoding\n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# Perform one hot encoding on label\n",
        "def one_hot_encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation image label array to one-hot format by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "    \n",
        "# Perform reverse one-hot-encoding on labels / preds\n",
        "def reverse_one_hot(image):\n",
        "    \"\"\"\n",
        "    Transform a 2D array in one-hot format (depth is num_classes),\n",
        "    to a 2D array with only 1 channel, where each pixel value is\n",
        "    the classified class key.\n",
        "    # Arguments\n",
        "        image: The one-hot format image \n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of 1, where each pixel value is the classified \n",
        "        class key.\n",
        "    \"\"\"\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    \"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrQnidaabS2E"
      },
      "source": [
        "class BuildingsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    \"\"\"Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            class_rgb_values=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        \n",
        "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
        "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
        "\n",
        "        self.class_rgb_values = class_rgb_values\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read images and masks\n",
        "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # one-hot-encode the mask\n",
        "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return length of \n",
        "        return len(self.image_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcdQkzzqbZ8l"
      },
      "source": [
        "# Visualize Sample Image and Mask\n",
        "dataset = BuildingsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\n",
        "random_idx = random.randint(0, len(dataset)-1)\n",
        "image, mask = dataset[random_idx]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8sua5nebgAm"
      },
      "source": [
        "# Defining Augmentations\n",
        "def get_training_augmentation():\n",
        "    train_transform = [    \n",
        "        album.RandomCrop(height=160, width=160, always_apply=False, p=1),\n",
        "        #album.OneOf(\n",
        "            #[\n",
        "                #album.HorizontalFlip(p=1),\n",
        "                #album.VerticalFlip(p=1),\n",
        "                #album.RandomRotate90(p=1),\n",
        "            #], p=0.5),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():   \n",
        "    # Add sufficient padding to ensure image is divisible by 32\n",
        "    test_transform = [\n",
        "        album.PadIfNeeded(min_height=192, min_width=192, always_apply=True, border_mode=0),\n",
        "    ]\n",
        "    return album.Compose(test_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    \"\"\"Construct preprocessing transform    \n",
        "    Args:\n",
        "        preprocessing_fn (callable): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \"\"\"   \n",
        "    _transform = []\n",
        "    if preprocessing_fn:\n",
        "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
        "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
        "        \n",
        "    return album.Compose(_transform)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P2bEIxNbj5Q"
      },
      "source": [
        "#Visualize Augmented Images & Masks\n",
        "augmented_dataset = BuildingsDataset(\n",
        "    x_train_dir, y_train_dir, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
        "\n",
        "# Different augmentations on a random image/mask pair (128*128 crop)\n",
        "for i in range(3):#3 firsts\n",
        "    image, mask = augmented_dataset[random_idx]\n",
        "    visualize(\n",
        "        original_image = image,\n",
        "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
        "    )\n",
        "\n",
        "print(image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPJCYpTuboNI"
      },
      "source": [
        "#Model Definition\n",
        "ENCODER = 'resnet34'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "ENCODER_DEPTH=3\n",
        "CLASSES = class_names\n",
        "ACTIVATION = 'softmax2d' #'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.UnetPlusPlus(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_depth=ENCODER_DEPTH,\n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    decoder_use_batchnorm=True,\n",
        "    decoder_channels=(512, 256, 128),#(512, 256, 128, 64, 64),#same length as encoder_depth\n",
        "    in_channels=3,#rgb\n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqsB-dwXONUp"
      },
      "source": [
        "#from torchsummary import summary\n",
        "#summary(model, (3, 160, 160))\n",
        "#model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11YptxCVbvRw"
      },
      "source": [
        "#!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n",
        "# Get train and val dataset instances\n",
        "train_dataset = BuildingsDataset(\n",
        "    x_train_dir, y_train_dir, \n",
        "    augmentation= get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ") \n",
        "\n",
        "print('len_train_dataset: ',len(train_dataset))\n",
        "\n",
        "valid_dataset = BuildingsDataset(\n",
        "    x_valid_dir, y_valid_dir, \n",
        "    augmentation= get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "print('len_valid_dataset: ',len(valid_dataset))\n",
        "\n",
        "# Get train and val data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=6, drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, num_workers=2, drop_last=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqAQQRHzby-J"
      },
      "source": [
        "# Set Hyperparams\n",
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "TRAINING = True\n",
        "\n",
        "# Set num of epochs\n",
        "EPOCHS = 200\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define loss function\n",
        "#loss = smp.utils.losses.constants.MULTICLASS_MODE()\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "\n",
        "# define metrics\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.1, weight_decay=0.006),])#0.015\n",
        "\n",
        "# define learning rate scheduler \n",
        "\n",
        "lmbda = lambda epoch: 0.958 #0.65** epoch\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "if os.path.exists('best_model_previous_run/best_model.pth'):\n",
        "    model = torch.load('best_model_previous_run/best_model.pth', map_location=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi_JZgBPb2mm"
      },
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iwRPItgb7hW"
      },
      "source": [
        "# Training DeepLabV3+\n",
        "%%time\n",
        "\n",
        "if TRAINING:\n",
        "\n",
        "    best_iou_score = 0.0\n",
        "    train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "    for i in range(0, EPOCHS):\n",
        "\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step(i)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            print(\"LEARNING RATE: \", param_group['lr'])\n",
        "            usedLr = float(param_group['lr'])\n",
        "\n",
        "        # Perform training & validation\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(valid_loader)\n",
        "        train_logs_list.append(train_logs)\n",
        "        valid_logs_list.append(valid_logs)\n",
        "\n",
        "        # Save model if a better val IoU score is obtained\n",
        "        if best_iou_score < valid_logs['iou_score']:\n",
        "            best_iou_score = valid_logs['iou_score']\n",
        "            torch.save(model, 'best_model_current_run/best_model.pth')\n",
        "            print('Model saved!')\n",
        "\n",
        "        # evaluate on the test dataset\n",
        "        # evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orwxB8QBcBTL"
      },
      "source": [
        "# Prediction on Test Data\n",
        "# load best saved model checkpoint from the current run\n",
        "if os.path.exists('best_model_current_run/best_model.pth'):\n",
        "    best_model = torch.load('best_model_current_run/best_model.pth', map_location=DEVICE)\n",
        "    print('Loaded Unet++ model from this run.')\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "elif os.path.exists('best_model_previous_run/best_model.pth'):\n",
        "    best_model = torch.load('best_model_previous_run/best_model.pth', map_location=DEVICE)\n",
        "    print('Loaded Unet++ model from a previous commit.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us7WY_q8cFf9"
      },
      "source": [
        "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
        "test_dataset = BuildingsDataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)\n",
        "\n",
        "# test dataset for visualization (without preprocessing transformations)\n",
        "test_dataset_vis = BuildingsDataset(\n",
        "    x_test_dir, y_test_dir, \n",
        "    augmentation=get_validation_augmentation(),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "# get a random test image/mask index\n",
        "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
        "image, mask = test_dataset_vis[random_idx]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X89ImffMcR5L"
      },
      "source": [
        "# Center crop padded image / mask to original image dims\n",
        "#def crop_image(image, target_image_dims=[1500,1500,3]):\n",
        "def crop_image(image, target_image_dims=[160,160,3]): \n",
        "    target_size = target_image_dims[0]\n",
        "    image_size = len(image)\n",
        "    padding = (image_size - target_size) // 2\n",
        "\n",
        "    return image[\n",
        "        padding:image_size - padding,\n",
        "        padding:image_size - padding,\n",
        "        :,\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLF_44i1cVbf"
      },
      "source": [
        "sample_preds_folder = 'sample_predictions/'\n",
        "if not os.path.exists(sample_preds_folder):\n",
        "    os.makedirs(sample_preds_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9EVxRppcag4"
      },
      "source": [
        "for idx in range(0, len(test_dataset), 4):#, 4\n",
        "\n",
        "    image, gt_mask = test_dataset[idx]\n",
        "    image_vis = crop_image(test_dataset_vis[idx][0].astype('uint8'))\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    # Predict test image\n",
        "    pred_mask = best_model(x_tensor)\n",
        "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "    # Convert pred_mask from `CHW` format to `HWC` format\n",
        "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
        "    # Get prediction channel corresponding to building\n",
        "    pred_grainmap = pred_mask[:,:,select_classes.index('grain')]\n",
        "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n",
        "    # Convert gt_mask from `CHW` format to `HWC` format\n",
        "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
        "    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n",
        "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.tif\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
        "    \n",
        "    visualize(\n",
        "        original_image = image_vis,\n",
        "        ground_truth_mask = gt_mask,\n",
        "        predicted_mask = pred_mask,\n",
        "        predicted_grain = pred_grainmap\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIlqswJMcfYc"
      },
      "source": [
        "# Model Evaluation on Test Dataset\n",
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_logs = test_epoch.run(test_dataloader)\n",
        "print(\"Evaluation on Test Data: \")\n",
        "print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n",
        "print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjGfJy8Qci4x"
      },
      "source": [
        "#Plot Dice Loss & IoU Metric for Train vs. Val\n",
        "train_logs_df = pd.DataFrame(train_logs_list)\n",
        "valid_logs_df = pd.DataFrame(valid_logs_list)\n",
        "train_logs_df.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DSj7NyQcnef"
      },
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('IoU Score', fontsize=20)\n",
        "plt.title('IoU Score Plot', fontsize=20)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('iou_score_plot.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTkJy-B8cof_"
      },
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('Dice Loss', fontsize=20)\n",
        "plt.title('Dice Loss Plot', fontsize=20)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('dice_loss_plot.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN-r-mrnkQbV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VW7TkBM0Qyf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}